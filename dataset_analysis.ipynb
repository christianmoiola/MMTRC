{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giallo Zafferano Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianmoiola/VS Code/MMTRC/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing zero length recipes from english_N1000\n",
      "Removed 1 recipes\n",
      "Removing zero length recipes from italian_from_eng_N1000\n",
      "Removed 1 recipes\n",
      "Removing zero length recipes from italian_N50\n",
      "Removed 0 recipes\n",
      "Removing zero length recipes from italian_N1000\n",
      "Removed 8 recipes\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "english_N1000 = load_dataset('dataset/english_N1000_2025-02-11.json')\n",
    "italian_from_eng_N1000 = load_dataset('dataset/italian_from_eng_N1000_2025-02-11.json')\n",
    "italian_N50 = load_dataset('dataset/italian_N50_2024-11-29.json')\n",
    "italian_N1000 = load_dataset('dataset/italian_N1000_2025-02-11.json')\n",
    "\n",
    "dataset = {\n",
    "    'english_N1000': english_N1000,\n",
    "    'italian_from_eng_N1000': italian_from_eng_N1000,\n",
    "    'italian_N50': italian_N50,\n",
    "    'italian_N1000': italian_N1000\n",
    "}\n",
    "\n",
    "def remove_zero_length_receipes(data):\n",
    "    \"\"\"\n",
    "    Remove recipes with zero length\n",
    "    \"\"\"\n",
    "    return [recipe for recipe in data if len(recipe[\"steps\"]) > 0]\n",
    "\n",
    "for name, data in dataset.items():\n",
    "    print(f\"Removing zero length recipes from {name}\")\n",
    "    dataset[name] = remove_zero_length_receipes(data)\n",
    "    print(f\"Removed {len(data) - len(dataset[name])} recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of recipes in each partiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipes in english_N1000 partition: 670\n",
      "Number of recipes in italian_from_eng_N1000 partition: 685\n",
      "Number of recipes in italian_N50 partition: 32\n",
      "Number of recipes in italian_N1000 partition: 629\n"
     ]
    }
   ],
   "source": [
    "for name, partition in dataset.items():\n",
    "    print(f\"Number of recipes in {name} partition: {len(partition)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min, Max, Mean and Standard Deviation of steps in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_N1000 partition: min: 6, max: 45, mean: 19.4955223880597, std: 6.961245231628599 of steps\n",
      "italian_from_eng_N1000 partition: min: 6, max: 48, mean: 19.541605839416057, std: 7.087365568809845 of steps\n",
      "italian_N50 partition: min: 6, max: 39, mean: 21.0, std: 8.147085368400163 of steps\n",
      "italian_N1000 partition: min: 3, max: 42, mean: 19.211446740858506, std: 7.431176279419841 of steps\n"
     ]
    }
   ],
   "source": [
    "def min_max_mean_std_steps(dataset):\n",
    "    mean_length = np.mean([len(el[\"steps\"]) for el in dataset])\n",
    "    max_length = np.max([len(el[\"steps\"]) for el in dataset])\n",
    "    min_length = np.min([len(el[\"steps\"]) for el in dataset])\n",
    "    standard_deviation = np.std([len(el[\"steps\"]) for el in dataset])\n",
    "    return min_length, max_length, mean_length, standard_deviation\n",
    "\n",
    "for name, partition in dataset.items():\n",
    "    min_length, max_length, mean_length, std_length = min_max_mean_std_steps(partition)\n",
    "    print(f\"{name} partition: min: {min_length}, max: {max_length}, mean: {mean_length}, std: {std_length} of steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of input examples (Sj+1) (Sj+2) (Sj+5) for each partiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_N1000 partition: sj1: 12392, sj2: 11722, sj5: 9712\n",
      "italian_from_eng_N1000 partition: sj1: 12701, sj2: 12016, sj5: 9961\n",
      "italian_N50 partition: sj1: 640, sj2: 608, sj5: 512\n",
      "italian_N1000 partition: sj1: 11455, sj2: 10826, sj5: 8943\n"
     ]
    }
   ],
   "source": [
    "for name, partition in dataset.items():\n",
    "    sj1 = 0\n",
    "    sj2 = 0\n",
    "    sj5 = 0\n",
    "    for el in partition:\n",
    "        sj1 += len(el[\"steps\"]) - 1 if len(el[\"steps\"]) > 1 else 0\n",
    "        sj2 += len(el[\"steps\"]) - 2 if len(el[\"steps\"]) > 2 else 0\n",
    "        sj5 += len(el[\"steps\"]) - 5 if len(el[\"steps\"]) > 5 else 0\n",
    "    \n",
    "    print(f\"{name} partition: sj1: {sj1}, sj2: {sj2}, sj5: {sj5}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianmoiola/VS Code/MMTRC/venv/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing english_N1000 partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "en Tokenizing steps...: 100%|██████████| 670/670 [00:39<00:00, 17.13recipe/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for english_N1000 partition.\n",
      "Tokenizing italian_from_eng_N1000 partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it Tokenizing steps...: 100%|██████████| 685/685 [00:40<00:00, 17.01recipe/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for italian_from_eng_N1000 partition.\n",
      "Tokenizing italian_N50 partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it Tokenizing steps...: 100%|██████████| 32/32 [00:02<00:00, 14.98recipe/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for italian_N50 partition.\n",
      "Tokenizing italian_N1000 partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "it Tokenizing steps...: 100%|██████████| 629/629 [00:37<00:00, 16.80recipe/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete for italian_N1000 partition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_eng = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ita = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "def tokenize_text(text, language='en'):\n",
    "    '''\n",
    "    Tokenizes the text using spaCy and removes stop words and punctuation.\n",
    "    '''\n",
    "    if language == 'en':\n",
    "        doc = nlp_eng(text[\"text\"])\n",
    "    elif language == 'it':\n",
    "        doc = nlp_ita(text[\"text\"])\n",
    "    else:\n",
    "        raise ValueError(\"Language not supported. Use 'en' for English or 'it' for Italian.\")\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
    "    return tokens\n",
    "\n",
    "def tokenize_dataset(dataset, name):\n",
    "    '''\n",
    "    Tokenizes the steps in recipes using spaCy.\n",
    "    The function takes a dataset as input and returns the dataset with tokenized steps as a new field.\n",
    "    '''\n",
    "    if name.startswith(\"italian\"):\n",
    "        language = 'it'\n",
    "    elif name.startswith(\"english\"):\n",
    "        language = 'en'\n",
    "    else:\n",
    "        raise ValueError(\"Language not supported. Use 'english' or 'italian'.\")\n",
    "    for el in tqdm(dataset, desc=f\"{language} Tokenizing steps...\", unit=\"recipe\"):\n",
    "        el[\"tokenized_steps\"] = [tokenize_text(step, language) for step in el[\"steps\"]]\n",
    "    return dataset\n",
    "\n",
    "for name, partition in dataset.items():\n",
    "    print(f\"Tokenizing {name} partition...\")\n",
    "    dataset[name] = tokenize_dataset(partition, name)\n",
    "    print(f\"Tokenization complete for {name} partition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min, Max, Standard Deviation of tokens for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_N1000 partition: min: 1, max: 100, mean: 6.260529344073648, std: 4.532229064728398 of tokens for steps\n",
      "italian_from_eng_N1000 partition: min: 1, max: 49, mean: 6.532112845138055, std: 4.479178986759654 of tokens for steps\n",
      "italian_N50 partition: min: 1, max: 33, mean: 7.39344262295082, std: 5.065021504676269 of tokens for steps\n",
      "italian_N1000 partition: min: 1, max: 51, mean: 6.8323089700996675, std: 4.652442217687702 of tokens for steps\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(dataset):\n",
    "    '''\n",
    "    Counts the number of tokens in each recipe's steps.\n",
    "    The function takes a dataset as input and returns the dataset with token counts as a new field.\n",
    "    '''\n",
    "    for el in dataset:\n",
    "       el[\"token_count\"] = [len(step) for step in el[\"tokenized_steps\"] if len(step) > 0]\n",
    "    return dataset\n",
    "\n",
    "for name, partition in dataset.items():\n",
    "    dataset[name] = count_tokens(partition)\n",
    "\n",
    "def min_max_mean_std_tokens_steps(dataset):\n",
    "    '''\n",
    "    Computes the minimum, maximum, mean, and standard deviation of token counts in the dataset.\n",
    "    The function takes a dataset as input and returns these statistics.\n",
    "    '''\n",
    "    mean_length = np.mean([step for el in dataset for step in el[\"token_count\"]])\n",
    "    max_length = np.max([step for el in dataset for step in el[\"token_count\"]])\n",
    "    min_length = np.min([step for el in dataset for step in el[\"token_count\"]])\n",
    "    standard_deviation = np.std([step for el in dataset for step in el[\"token_count\"]])\n",
    "    return min_length, max_length, mean_length, standard_deviation\n",
    "\n",
    "for name, partition in dataset.items():\n",
    "    min_length, max_length, mean_length, std_length = min_max_mean_std_tokens_steps(partition)\n",
    "    print(f\"{name} partition: min: {min_length}, max: {max_length}, mean: {mean_length}, std: {std_length} of tokens for steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min, Max, Standard Deviation of tokens for each receipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_N1000 partition: min: 30, max: 344, mean: 121.8, std: 48.9704327485687 of tokens for recipes\n",
      "italian_from_eng_N1000 partition: min: 30, max: 381, mean: 127.0948905109489, std: 52.56721567359397 of tokens for recipes\n",
      "italian_N50 partition: min: 42, max: 308, mean: 155.03125, std: 64.32898082075839 of tokens for recipes\n",
      "italian_N1000 partition: min: 23, max: 360, mean: 130.7806041335453, std: 55.25762359722652 of tokens for recipes\n"
     ]
    }
   ],
   "source": [
    "def min_max_mean_std_tokens_recipes(dataset):\n",
    "    '''\n",
    "    Computes the minimum, maximum, mean, and standard deviation of token counts in the recipes.\n",
    "    The function takes a dataset as input and returns these statistics.\n",
    "    '''\n",
    "    mean_length = np.mean([sum(el[\"token_count\"])for el in dataset])\n",
    "    max_length = np.max([sum(el[\"token_count\"])for el in dataset])\n",
    "    min_length = np.min([sum(el[\"token_count\"])for el in dataset])\n",
    "    standard_deviation = np.std([sum(el[\"token_count\"])for el in dataset])\n",
    "    return min_length, max_length, mean_length, standard_deviation\n",
    "\n",
    "for name, partition in dataset.items():\n",
    "    min_length, max_length, mean_length, std_length = min_max_mean_std_tokens_recipes(partition)\n",
    "    print(f\"{name} partition: min: {min_length}, max: {max_length}, mean: {mean_length}, std: {std_length} of tokens for recipes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribuiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in english_N1000 partition: [('add', 1669), ('minutes', 1340), ('water', 853), ('heat', 763), ('oil', 708), ('salt', 686), ('pan', 686), ('bowl', 655), ('prepare', 627), ('Add', 576)]\n",
      "Most common words in italian_from_eng_N1000 partition: [('i', 1620), ('minuti', 1252), ('l’', 931), ('acqua', 855), ('aggiungete', 787), ('olio', 711), ('versate', 680), ('cottura', 632), ('ciotola', 597), ('forno', 587)]\n",
      "Most common words in italian_N50 partition: [('i', 106), ('minuti', 74), ('l’', 66), ('acqua', 43), ('aggiungete', 41), ('composto', 36), ('forno', 34), ('ciotola', 33), ('olio', 33), ('cottura', 30)]\n",
      "Most common words in italian_N1000 partition: [('i', 1760), ('minuti', 1238), ('l’', 922), ('acqua', 756), ('aggiungete', 718), ('olio', 628), ('versate', 620), ('forno', 616), ('ciotola', 579), ('composto', 540)]\n"
     ]
    }
   ],
   "source": [
    "def frequency_distribution(dataset):\n",
    "    '''\n",
    "    Computes the frequency distribution of tokenized steps in the dataset.\n",
    "    The function takes a dataset as input and returns the frequency distribution.\n",
    "    '''\n",
    "    token_counter = Counter()\n",
    "    for el in dataset:\n",
    "        for step in el[\"tokenized_steps\"]:\n",
    "            token_counter.update(step)\n",
    "    return token_counter\n",
    "\n",
    "    \n",
    "for name, partition in dataset.items():\n",
    "    print(f\"Most common words in {name} partition: {frequency_distribution(partition).most_common(10)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
